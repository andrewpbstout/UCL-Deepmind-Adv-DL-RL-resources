{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL4_Q.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "11EWqwo1YwHBezgDoui106nTUXji_Dv7Q",
          "timestamp": 1520965749770
        },
        {
          "file_id": "1RHkaH9Jnw3Wx-zMz91n3DdP0Nls5Q4cF",
          "timestamp": 1520684960888
        },
        {
          "file_id": "1rIaB6u2aCySDUkP0qdFyfZl0_tOLGWGb",
          "timestamp": 1518445000610
        },
        {
          "file_id": "1OsB2DpNo1-NciDOlB5Vrk2YE7zMl8Hds",
          "timestamp": 1518116978983
        },
        {
          "file_id": "1yFj1LZ7MPleNtBFX6NS9i1zwZ74huXrQ",
          "timestamp": 1517308250935
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "GHl6N3Xfkctm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Homework 4\n",
        "\n",
        "-------------------------------\n",
        "\n",
        "\n",
        "**Name:** Your Name\n",
        "\n",
        "**SN:** Your Student Number\n",
        "\n",
        "-----------------------------------\n",
        "\n",
        "\n",
        "**Start date:** *13th March 2018*\n",
        "\n",
        "**Due date:** *29th March 2018, 11:55 pm*\n",
        "\n",
        "------------------------------------\n",
        "\n",
        "## How to Submit\n",
        "\n",
        "When you have completed the exercises and everything has finsihed running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **studentnumber_DL_hw4.ipynb** before the deadline above.\n",
        "\n",
        "Also send a **sharable link** to the notebook at the following email: ucl.coursework.submit@gmail.com. You can also make it sharable via link to everyone, up to you.\n",
        "\n",
        "Please compile all results, all plots/figures and all answers to the understanding/analysis results questions into a PDF. Name convention: **studentnumber_DL_hw4.pdf**. Do not include any of the code (we will use the notebook for that). \n",
        "\n",
        "**Page limit: 15 pg **.\n",
        "\n",
        "------------------------------------\n",
        "\n",
        "## PART 1: MNIST as a sequence (follow-up from last assignment)\n",
        "In this assignment we will be using the [MNIST digit dataset](https://yann.lecun.com/exdb/mnist/). The dataset contains images of hand-written digits ($0-9$), and the corresponding labels. The images have a resolution of $28\\times 28$ pixels. This is the same dataset as in Assignment 1, but we will be using this data a bit differently this time around. Since this assignment will be focusing on recurrent networks that model sequential data, we will be looking at each image as a sequence: the networks you train will be \"reading\" the image one row at a time, from top to bottom (we could even do pixel-by-pixel, but in the interest of time we'll do row-by-row which is faster).  Also, we will work with a binarized version of MNIST -- we constrain the values of the pixels to be either $0$ or $1$. You can do this by applying the method `binarize`, defined below, to the raw images.\n",
        "\n",
        "<img src=\"https://github.com/bodono/files/blob/master/mnist_as_sequence.png?raw=true\">\n",
        "\n",
        "* We take the MNIST images, binarise them, and interpret them as a sequence of pixels from top-left to bottom-right. (\"Task 2\" refers to the next homework, wherein you will be using the sequence for pixel prediction).\n",
        "\n",
        "## Recurrent Models for MNIST\n",
        "\n",
        "As discussed in the lectures, there are various ways and tasks for which we can use recurrent models. A depiction of the most common scenarios is available in the Figure below. In this assignment and the following one we will look at two of these forms: **many-to-one** (sequence to label/decision) and the **many-to-many** scenario where the model receives an input and produces an output at every time step. You will use these to solve the following tasks: i) pixel prediction  and ii) in-painting.\n",
        "\n",
        "<img src=\"https://github.com/bodono/files/blob/master/rnn_models.png?raw=true\">\n",
        "* ([Figure adapted from Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness)). You will be implementing variants of *many-to-one* for classification (in this homework), and *many-to-many* for prediction (in the next homework).\n"
      ]
    },
    {
      "metadata": {
        "id": "Dj6WmOJKYRTB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1: (Next) Pixel prediction (35 pts)\n",
        "In this part, you will train a **many-to-many** recurrent model: at each time $t$, the model receives as input a pixel value $x_t$ and tries to predict the next pixel in the images $x_{t+1}$ based on the current input and the recurrent state. Thus, your output function is now a probability over the value of pixel $x_{t+1}$ -- which can be either $0$ or $1$ (black or white).\n",
        "\\begin{equation}\n",
        "    \\hat{p}(x_{t+1}|x_{1:t}) = g(x_{t}, h_{t}, c_{t})\n",
        "\\end{equation}\n",
        "Once we get to observe the actual value of $x_{t+1}$ at the next time-step, we can compute the cross-entropy between our predicted probability $\\hat{p}(x_{t+1}|x_{1:t})$ and the observed value (pixel in the image). We can (and will) do that for every time-step prediction within a sequence. This will provide us with the training signal for optimizing the parameters of the mapping $g$ and the recurrent connections -- remember these are shared!, they do not change with $t$.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cqrTk9bLYTpp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimization\n",
        "Use the Adam optimizer (with default settings other than the learning rate) for training.\n",
        "\n",
        "**[Optional]** Sometimes dropout has been shown to be beneficial in training recurrent models, so feel free to use it or any other form of regularization that seems to improve performance. It might be also worth trying out batch-normalization. [Reference](https://arxiv.org/pdf/1603.09025.pdf).\n",
        "\n",
        "### Models: Your models will have the following structure:\n",
        "1. [(Red Block)] The *input* (current binarised row of pixels) can be fed directly into the recurrent connection without  much further pre-processing.\n",
        "2. [(Blue Block)] The *output* (probabilities over the activation of the pixel) is produced by looking at the last output of the recurrent units, transforming them via an affine transformation.\n",
        "3. [(Green Block)] For the *recurrent* part of the network, please implement and compare the following architectures:\n",
        "    * LSTM with 32 units. **[15 pts]**\n",
        "    \n",
        "    **OR**\n",
        "    \n",
        "    * GRU with 32 units. **[15 pts]**\n",
        "\n",
        "Your network should look like:\n",
        "\\begin{equation}\n",
        "\\textrm{Input} \\Rightarrow \\textrm{RNN cell} \\Rightarrow \\textrm{Relu} \\Rightarrow \\textrm{Fully connected} \\Rightarrow \\textrm{Relu} \\Rightarrow \\textrm{Fully connected} \\Rightarrow \\textrm{Output}\n",
        "\\end{equation}\n",
        "You might find the function `tf.nn.dynamic_rnn` useful.\n",
        "\n",
        "### Hyper-parameters \n",
        "For all cases train the model with these hyper-parameter settings:\n",
        "\n",
        "- *num_epochs*=5, *learning_rate*=0.001, *batch_size*=256, *fully_connected_hidden_units=64*\n",
        "\n",
        "With these hyper-parameters you should give you a good perfomance on both GRUs/LSTMs. It is worth noting that in $5$ epochs the model has yet converged, but in the interest of time (the training should have $\\approx$ 1h). That being said, feel free to try other settings, there are certainly better choices, but please report the results with these exact hyper-parameters and/or train for longer -- the models should still improve (convergence is achieved around 25-30 epochs). \n",
        "\n",
        "### Tasks:\n",
        "\n",
        "1) Implement and train the previously described model (choose either GRU **or** LSTM). Please report the *cross-entropy* on the *test set* and *training set* of the models trained. Use the `plot_summary_table` method below to format the table. Provide the learning curves (both training and testing loss) -- choose appropiate reporting interval here (at least 20 points).\n",
        "\n",
        "2) Using the previously trained model, visualize the 1-step predictions, 10-step predictions, one row prediction (28 steps) and filling out the image (fill out all the pixels using the recurrent model). \n",
        "\n",
        "*   **Generate a small in-painting dataset.** Sample $100$ images from your test set. Mask/Remove the last $300$ pixels (roughly 10 rows and a half).\n",
        "\n",
        "*   **Predict missing parts and compare with GT**. Given the above generated partial sequences as input to your train models, generate the continuation of these masked images (for the next 1, 10, 28, 300 pixels).\n",
        "Report the cross-entropy of your in-paintings for the trained model at beginning of training(0 epochs), after 1 epoch and at the end of training. Discuss the results: contrasting long/short time prediction; compare these with the cross-entropy of the ground truth images. For multiple steps in-paintings, average the loss over $10$ samples. **[10 pts]**\n",
        "\n",
        "*   **Visualize completing the image**. Pick out $3$ examples from your in-painting dataset to visualize  the resulting images -- this can be done at random, but should include *a successful example, failure example and one that displays high variance between samples.* For each example picked, please provide $5$ samples for the last three scenarios (10, 28, 300 pixels) and $1$ for the 1-pixel prediction -- total 16 samples/exampls. The samples should be generated recursively by sampling the generative process provided by the trained recurrent connections. Total number of in-painting to report: 16 samples x 3 examples = 48 **[10 pts]**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_xkdbD4mcfLk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 2: Using pixel-to-pixel: In-painting task (25 pts)\n",
        "\n",
        "Using the models trained in the previous section, please in-paint the missing pixels in the following datasets:\n",
        "* [One-pixel missing](https://github.com/dianaborsa/compgi22_dl_cw4/blob/master/one_pixel_inpainting.npy)\n",
        "* [Window of 2x2 pixels missing](https://github.com/dianaborsa/compgi22_dl_cw4/blob/master/2X2_pixels_inpainting.npy)\n",
        "\n",
        "This is similar to Task 1.b, but now you have information not only about the past(previous pixels in the image) but also future (pixels that come after your predictive target)\n",
        "\n",
        "### Results\n",
        "1) Provide the formula used to compute the probability over the missing pixel and respectively for the missing patch **[5+5 pts]**\n",
        "\n",
        "2) Visualize the most probable in-painting, according to your model. How does this compare to the ground truth? (Compare cross-entropy between your most probable sample and the ground truth). Explain the difference. It is enough to include just one example per task/dataset. **[10 pts]**\n"
      ]
    },
    {
      "metadata": {
        "id": "72eL--uyop0z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### In-painting data \n",
        "\n",
        "We provide two datasets (one corresponing to the one-pixel in-painting tasks and the other one with a 2x2 patch missing). The datasets are available on [git](https://github.com/dianaborsa/compgi22_dl_cw4/blob/master/). Links are available in the description and code is provide below to load the dataset and visualize. Both datasets have 1000 sampled images from MNIST(test). Both dataset sets have the same simple structure: cropped images and their ground truth (GT). in this second task, you will consider the copped images and use your pixel-to-pixel model, try to predict the missing pixel/patches.\n"
      ]
    },
    {
      "metadata": {
        "id": "0wg-OYIMm8cW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports and utility functions (do not modify!)"
      ]
    },
    {
      "metadata": {
        "id": "rPYnfzQfltxp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "#@title Import libraries\n",
        "#@test {\"output\": \"ignore\"}\n",
        "\n",
        "# Import useful libraries.\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# Binarize the images\n",
        "def binarize(images, threshold=0.1):\n",
        "  return (threshold < images).astype('float32')\n",
        "\n",
        "# Import dataset with one-hot encoding of the class labels.\n",
        "def get_data():\n",
        "  return input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# Placeholders to feed train and test data into the graph.\n",
        "# Since batch dimension is 'None', we can reuse them both for train and eval.\n",
        "def get_placeholders():\n",
        "  x = tf.placeholder(tf.float32, [None, 783])\n",
        "  y = tf.placeholder(tf.float32, [None, 783])\n",
        "  return x, y\n",
        "\n",
        "# Generate summary table of results. This function expects a dict with the\n",
        "# following structure: keys of 'LSTM' (or 'GRU') and the values for each key are a\n",
        "# list of tuples consisting of (test_loss, test_accuracy), and the list is\n",
        "# ordered as the results from 0 epoch (beginning of training), 1 epoch, 5 epochs (or end of training):\n",
        "# {\n",
        "#  'LSTM': [(loss,acc), (loss, acc), (loss, acc)]\n",
        "# }\n",
        "def plot_summary_table(experiment_results):\n",
        "  # Fill Data.\n",
        "  cell_text = []\n",
        "  columns = ['(Beginning - 0 epochs)', '(Mid-training - 1 epoch)', '(End of training - 5 epochs)']\n",
        "  for k, v in experiment_results.iteritems():\n",
        "    rows = ['Test loss', 'Test accuracy']\n",
        "    cell_text=[[],[]]\n",
        "    for (l, _) in v:\n",
        "      cell_text[0].append(str(l))\n",
        "    for (_, a) in v:\n",
        "      cell_text[1].append(str(a))\n",
        "\n",
        "    fig=plt.figure(frameon=False)\n",
        "    ax = plt.gca()\n",
        "    the_table = ax.table(\n",
        "      cellText=cell_text,\n",
        "      rowLabels=rows,\n",
        "      colLabels=columns,\n",
        "      loc='center')\n",
        "    the_table.scale(2, 8)\n",
        "    # Prettify.\n",
        "    ax.patch.set_facecolor('None')\n",
        "    ax.xaxis.set_visible(False)\n",
        "    ax.yaxis.set_visible(False)\n",
        "    ax.text(-0.73, 0.9, k, fontsize=18)\n",
        "    \n",
        "    \n",
        "    \n",
        "def plot_learning_curves(training_loss, testing_loss):\n",
        "  plt.figure()\n",
        "  plt.plot(training_loss)\n",
        "  plt.plot(testing_loss, 'g')\n",
        "  plt.legend(['Training loss', 'Testing loss'])\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xm9jZeiYphO8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train Models\n",
        "\n",
        "Generate summary table of results. This function expects a dict with the\n",
        "following structure: keys of 'LSTM' (or 'GRU') and the values for each key are a\n",
        "list of tuples consisting of (test_loss, test_accuracy), and the list is\n",
        "ordered as the results from 0 epoch (beginning of training), 1 epoch, 5 epochs (or end of training) i.e. expected dictionary (final performace only):\n",
        "\n",
        "```python\n",
        "{\n",
        "  'LSTM': [(loss,acc), (loss, acc), (loss, acc)]\n",
        "}\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "vLHuJ7Ncq7Jm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "#@title Your models\n",
        "\n",
        "# Advisable to you GPU for this part\n",
        "with tf.device('/device:GPU:*'):\n",
        "\n",
        "  tf.reset_default_graph()\n",
        "  \n",
        "  # your model here.... "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MDSEh0VujBHe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "#@title Training\n",
        "#@test {\"output\": \"ignore\"}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hr-L9cZl37ek",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Results"
      ]
    },
    {
      "metadata": {
        "id": "CtDksStF35Vb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plot_summary_table(experiment_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BKNhs1sHoVZh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pixel prediction"
      ]
    },
    {
      "metadata": {
        "id": "KPI6CZ35sFzt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate a small in-painting dataset.\n",
        "Sample $100$ images from your test set. Mask/Remove the last $300$ pixels (roughly 10 rows and a half)."
      ]
    },
    {
      "metadata": {
        "id": "tjNLq2JjsNEd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Sample 100 images + mask "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8yO15xCYsONg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Predict missing parts and compare with the ground truth. \n",
        "Given the above generated partial sequences as input to your train models, generate the continuation of these masked images (for the next 1, 10, 28, 300 pixels)."
      ]
    },
    {
      "metadata": {
        "id": "vTgoUCyBsNok",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Use the model for predictions and compare log likelihood"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FC06V0nJsbz8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize completing the image. \n",
        "Pick out $3$ examples from your in-painting dataset to visualize  the resulting images -- this can be done at random, but should include \\textit{a successful example, failure example and one that displays high variance between samples.}"
      ]
    },
    {
      "metadata": {
        "id": "4yf7jlPWsbZ8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Visualize samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kuHRuG8nr1Ne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# In-painting Task"
      ]
    },
    {
      "metadata": {
        "id": "ZDPntgyZjPWO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### In-painting data \n",
        "\n",
        "We provide two datasets (one corresponing to the one-pixel in-painting tasks and the other one with a 2x2 patch missing). The datasets are available on [git](https://github.com/dianaborsa/compgi22_dl_cw4/blob/master/). Links are available in the description and code is provide below to load the dataset and visualize. Both datasets have 1000 sampled images from MNIST(test). Both dataset sets have the same simple structure: cropped images and their ground truth (GT). in this second task, you will consider the copped images and use your pixel-to-pixel model, try to predict the missing pixel/patches.\n"
      ]
    },
    {
      "metadata": {
        "id": "ur-UmPh9ce4N",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2f4a1f9c-cd30-4196-e490-266188fbec0f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520968282790,
          "user_tz": 0,
          "elapsed": 931,
          "user": {
            "displayName": "Diana Borsa",
            "photoUrl": "//lh3.googleusercontent.com/-ncAFLYhkpa4/AAAAAAAAAAI/AAAAAAAABCo/oObvci_cgWA/s50-c-k-no/photo.jpg",
            "userId": "113702420363391077417"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading the inpainting datasets\n",
        "!git clone https://github.com/dianaborsa/compgi22_dl_cw4.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'compgi22_dl_cw4' already exists and is not an empty directory.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XD0JMYbqb9t_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the dataset (2X2)\n",
        "dataset = np.load('compgi22_dl_cw4/2X2_pixels_inpainting.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ftjlfCA-cY2X",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {},
            {},
            {},
            {}
          ],
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "outputId": "ea3dbb9d-2f81-4404-d4e5-f8c9e05d47df",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520964508112,
          "user_tz": 0,
          "elapsed": 986,
          "user": {
            "displayName": "Diana Borsa",
            "photoUrl": "//lh3.googleusercontent.com/-ncAFLYhkpa4/AAAAAAAAAAI/AAAAAAAABCo/oObvci_cgWA/s50-c-k-no/photo.jpg",
            "userId": "113702420363391077417"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# checking loading\n",
        "images    = dataset[0]\n",
        "gt_images = dataset[1] \n",
        "\n",
        "nSamples, ndim = gt_images.shape\n",
        "print('Loaded dataset has {} samples: cropped + GT'.format(nSamples))\n",
        "\n",
        "# randomly visualize a few samples\n",
        "for SampleID in np.random.randint(nSamples,size=3):\n",
        "  plt.figure()\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(np.reshape(gt_images[SampleID],(28,28)), interpolation='None',vmin=-1, vmax=1)\n",
        "  plt.title(\"GT image\")\n",
        "  plt.grid(False)\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(np.reshape(images[SampleID],(28,28)), interpolation='None',vmin=-1, vmax=1)\n",
        "  plt.title(\"Cropped image\")\n",
        "  plt.grid(False)\n",
        "  plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded dataset has 1000 samples: cropped + GT\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAD5CAYAAABmgj/HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f34ab69a750>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAD5CAYAAABmgj/HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f34ab601610>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAD5CAYAAABmgj/HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f34ab5cf2d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "G9RAXOberfI9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### One-pixel prediction "
      ]
    },
    {
      "metadata": {
        "id": "PGeeQQMZrjpu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Predicting the missing pixel\n",
        "\n",
        "# Visualize samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BfAXkujUtJ3O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Two-by-two patch prediction "
      ]
    },
    {
      "metadata": {
        "id": "9YayTWkNtJ3P",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Predicting the missing patch\n",
        "\n",
        "# Visualize samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYl1qOnDuwyx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3-nWwpWryjBO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------\n",
        "\n",
        "## PART 2: Learning multiple tasks with LSTM-s (40 pts)\n",
        "(Credits to Pedro Ortega for insipring the task and insights behind it)"
      ]
    },
    {
      "metadata": {
        "id": "xUSn2r_Q1ahS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Task Description\n",
        "\n",
        "Consider the following generative model: \n",
        "* We have $3$ symbols that will be generates from a multinomial/categorical distribution, with parameters $\\textbf{p}=(p_1, p_2, p_3)$: symbol 1 is generated with probability $p_1$, symbol 2 is generated with probability $p_2$, symbol 3 with probability $p_3$. \n",
        "$$X \\sim Categorical(3, \\textbf{p})$$\n",
        "\n",
        "* The parameter vector $\\textbf{p}$ is drawn from a Dirichlet prior: $$\\textbf{p} \\sim Dirichlet(\\alpha)$$\n",
        "\n",
        "We are going to use the above to generate sequences (a continuous stream of data/observations), in the following way:\n",
        "* Step 1: We sample $\\textbf{p}$ from the prior\n",
        "* Step 2: Given this $\\textbf{p}$, for $T-1$ time-steps we will generate i.i.d observations by sampling one of the $3$ symbols from the categorical distribution induced by $\\textbf{p}$\n",
        "$$ (X_1, X_2, \\cdots X_{T-1}) , s.t. X_i \\sim Categorical(3, \\textbf{p})$$\n",
        "* Step 3: At the end of the sequence we append a fourth symbol (RESET): $(X_1, X_2, \\cdots X_T, RESET)$\n",
        "* Step 4: Return to Step 1 and resample $\\textbf{p}$. \n",
        "* Repeat this 'forever'.\n",
        "\n",
        "This will give rise to a continuous stream of data, of the form: $ x_1, x_2, \\cdots x_{T-1}, RESET, x_{T+1}, x_{T+2}, \\cdots x_{2T}, RESET, \\cdots, x_{kT+1}, x_{kT+2}, \\cdots x_{(k+1)T}, RESET, \\cdots$.\n",
        "\n",
        "Note: Data generation is provided for you in the cell below. (You just need to call the minibatch function to get a sequence of this form).\n",
        "\n",
        "### Model\n",
        "\n",
        "We are going to consider an simple LSTM (32 units hidden state) and present this (generated) sequence of data as an input. Similar to the pixel-to-pixel model, at each time step the LSTM will receive one bit of information (gets to observe the symbol recorded at this time step) and need to output the probability distribution for the next symbol. Thus, at time $t$ the LSTM get as input the symbol $x_t$ and will return a probability over the next state $P(x_{t+1}| x_{t}, LSTM_{t-1})$.\n",
        "\n",
        "### Questions\n",
        "\n",
        "1) **Without running any experiments (5 pts)**, try to think about the following scenarios and answer these questions: \n",
        "* Consider we generate the data with $Dirichlet(\\alpha)$, where $\\alpha = (10.,1.,1.)$. What do you think the LSTM model will learn, if anything? Remember we are effectively changing the prediction task, every time we are resampling the probability vector $\\textbf{p}$. * Hint: Think about the distribution over $\\textbf{p}$ that this prior induces. *  \n",
        "\n",
        "* What if we consider a more uniformative prior, like $\\alpha=(1.1, 1.1, 1.1)$?  \n",
        "\n",
        "* How does this (learning ability and generalization) depend on the lenght of the tasks $T$ and the unrolling length on the LSTM? It might be helpful to consider the two extremes: \n",
        "i) $T=1$ (we reset the task at every time step). What should the model learn in this case?, ii) $T=\\infty$ (we sample the task once and keep it forever). What should the model learn in this case? (Answer this for both previous priors)\n",
        "\n",
        "\n",
        "* Does this increase or descrease the complexity of the prediction problem? What about the ability to generalize to unseen $\\textbf{p}$?\n",
        "\n",
        "2) **Time to check your intuitions (10 pts)** \n",
        "\n",
        "Implement a similar LSTM model as in PART 1. This will take as input a one-hot description of the obsevation ($[1,0,0,0]$ for symbol 1. $[0,1,0,0]$ for symbol 2, $[0,0,1,0]$ for symbol 3, $[0,0,0,1]$ for the RESET symbol). This input is fed into a 32-unit LSTM and LSTM output is processed as before: $\\textrm{Relu} \\Rightarrow \\textrm{Fully connected} \\Rightarrow \\textrm{Relu} \\Rightarrow \\textrm{Fully connected} \\Rightarrow \\textrm{Output}$ . The model will be trained, as before, by cross-entropy on predicting the next symbol. You will notice that the setup is really similar to the previous tasks, so feel free to re-use whenever appropiate.\n",
        "\n",
        "Train the following models:\n",
        "* T = 5, and T=20 with the data generated from a Dirichlet with $\\alpha = (10.,1.,1.)$. Unrolling length for the LSTM = 100. Minibatch size = 64. (M1, M2)\n",
        "* T = 5, and T=20 with the data generated from a Dirichlet with$\\alpha = (1.3, 1.3, 1.3)$ Unrolling length for the LSTM = 100. Minibatch size = 64. (M3, M4)\n",
        "\n",
        "Train the models for $1000$ iterations ($1000$ minibatches). Record the training and testing preformance (every 10-20 iterations). Plot the curves over training time. What do you observe? (Is this curve smooth? Do any of them plateau?). **[2x5 pts]**\n",
        "\n",
        "\n",
        "3) **Analysis results (10 pts)**\n",
        "\n",
        "In this section, we will investigate what the models have actaully learnt. For this we will generate a few test sequences:\n",
        "* *Test sequence 1*: generate a test sequence that changes tasks every T=5 steps from a a Dirichlet with $\\alpha = (10.,1.,1.)$.\n",
        "* *Test sequence 2*: generate a test sequence that changes tasks every T=5 steps from a a Dirichlet with $\\alpha = (1.3,1.3,1.3)$.\n",
        "* *Test sequence 3*: generate a test sequence that 'changes tasks' every T=5 steps, but keep sampling according to the same probability vector $\\textbf{p}=(1,0,0)$ (You can use any of the extreme here).\n",
        "\n",
        "i) Test the preformance of M1 and M3 and these test sequences. In addition plot the actual prediction the models do (probability of symbols over time). This should give you more insight in what the model does. Does this correspond or contradict your previous intuitions? **[5 pts]**\n",
        "\n",
        "ii) Repeat the same procedure for task length $T=20$ and models M2 and M4. What do you observe? How do M2 and M4 compare to each other and how to their compare to M1 and M3 (the models trained on the shorter task length). **[5 pts]**\n",
        "\n",
        "4) **Comparison to the Bayesian update (15 pts)**\n",
        "\n",
        "Going back to the generative process in the task description. For a given prior, for each the mini-tasks (selecting/sampling a $\\textbf{p}$), one could compute the Bayesian posterior at each time step.  We start with a prior and every time we observe a symbol with update our posterior over the parameters\n",
        " $\\textbf{p}$ given the data. We do this every time step, till we reach the RESET symbol which marks the end of the task. Then we start again, from the prior.\n",
        "\n",
        "i) Derive the posterior update for each time step. (Hint: since the two distribution are conjugates or each other, the posterior has a closed form). **[3 pts]**\n",
        "\n",
        "ii) Implement this posterior update and use it to infer the probabilities over the next symbol, for the previously generated test sequences. This will tell you, what the inferred probabilities would be, if we knew the structure of the model, the prior and that the reset symbol means the tasks has finished and we should reset our estimate to the prior. (For test sequence 1 and 2, use the prior that generated them, for test sequence 3 compute the updates starting from both priors) **[5 pts]**\n",
        "\n",
        "iii) Compare this to what the LSTM predictions are. What do you observe? What are the failure cases -- can you explain why this might happen? (For test sequence 1 and 2, use the prior that generated them, for test sequence 3 compute the updates starting from both priors). **[7 pts]**\n",
        " \n",
        "For this analysis, only consider $T=20$ and respectively models M2 and M4.\n",
        "\n",
        "5) **Play (not for credit, just for fun)**\n",
        "\n",
        "Visualize the hidden state of the LSTM. Look at the eigenvalues: How many of these are actual revelant? What do they correspond to?\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7D0rGNa-ytbO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "#@title Generate data function\n",
        "\n",
        "n_symbols = 3\n",
        "def get_data_per_task(number_samples_per_task=10, p=None, alpha=None):\n",
        "  if p == None:\n",
        "    # sample task\n",
        "    if alpha == None:\n",
        "      p = np.random.dirichlet((1.3, 1.3, 1.3), 1)[0]\n",
        "    else:\n",
        "      p = np.random.dirichlet(alpha, 1)[0]\n",
        "    \n",
        "  p = np.append(p, [0])\n",
        "  sample = np.random.multinomial(1, p, size = number_samples_per_task)\n",
        "  \n",
        "  sample = np.append(sample,[[0,0,0,1]], axis=0)\n",
        "  return sample\n",
        "  \n",
        "\n",
        "def get_data(ntasks, nsamples_per_task, p=None, alpha=None):\n",
        "  sample = []\n",
        "  for task_id in range(ntasks):\n",
        "    sample.append(get_data_per_task(number_samples_per_task = nsamples_per_task, p=p, alpha=alpha))\n",
        "  return np.concatenate(sample)\n",
        "\n",
        "\n",
        "def get_minibatch(batch_size, ntasks, nsamples_per_task, p=None, alpha=None):\n",
        "  sample = get_data(batch_size*ntasks, nsamples_per_task, p=p, alpha=alpha)\n",
        "  return np.reshape(sample, [batch_size, ntasks*(nsamples_per_task+1), n_symbols+1])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YeVJ2gGdieG-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training your recurrent model"
      ]
    },
    {
      "metadata": {
        "id": "GpSBS2IKgZY3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "#@title Your models\n",
        "\n",
        "# Advisable to you GPU for this part\n",
        "with tf.device('/device:GPU:*'):\n",
        "\n",
        "  tf.reset_default_graph()\n",
        "  \n",
        "  # your model here.... "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zUGYk0AggZY8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "#@title Training\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WVa3hXcminFh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Analysing your recurrent model"
      ]
    },
    {
      "metadata": {
        "id": "1jhC7JmIghuz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Plot learning curves "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9-JWwb8EhXhi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Visualize performance on the test sequences:\n",
        "\n",
        "# Sample generation of test sequence:\n",
        "# get_minibatch(batch_size, num_tasks_per_batch, num_samples_per_task-1, p=[1.0, 0.0, 0.0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wSl5YYqCgm4D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Bayesian Updates"
      ]
    },
    {
      "metadata": {
        "id": "b_dhNHuSgpRM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Implement Bayesian update (as if you knew the 'right' prior and model)\n",
        "\n",
        "\n",
        "# From the posterior infer the mean of the probability vector. Use this as your \n",
        "# estimate to compare again the LSTM predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0PoqrDkoiKM5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Visualize and compare performance on the test sequences:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dy9v9HlziMuq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}